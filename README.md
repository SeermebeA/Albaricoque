# **Albaricoque â€“ Privacy-Preserving Human Detection Node (Edge AI Application Track)**

**Albaricoque** is a camera-free, privacy-preserving perimeter monitoring node capable of detecting **humans only** within **0â€“3 m** and a **~90Â° sector**, using hybrid **PIR + ultrasonic sensor fusion** and **TinyML inference** deployed on an **Arduino Nano 33 BLE Sense Rev2**.  
This project is submitted to the **HackerEarth Ã— Edge Impulse Hackathon 2025** under the **Edge AI Application Track**.

---

## ðŸ§­ **Project Summary**

Albaricoque runs all intelligence **on-device**, without cameras, cloud processing, or personal data collection. The system fuses directional PIR activation patterns with multi-angle ultrasonic distance deltas to build a unique motion signature that allows distinguishing **human presence** from **animals** and **environmental noise**.

<p align="center">
  <img src="https://github.com/SeermebeA/Albaricoque/blob/main/Multimedia/img/IMG_1844.jpg" width="450">
</p>


A classification model trained in **Edge Impulse** identifies six behavior classes (Animal, Solo, Walk1â€“Walk4), while an additional anomaly detector filters invalid sensor frames.

The system achieves **real-time inference (<1 ms)**, enabling deployment in low-power perimeter protection scenarios.

---

## ðŸŽ¯ **Problem Statement**

Most perimeter security systems depend on cameras or radar-like systems that:

- Violate privacy  
- Perform poorly at night or under rain  
- Require high bandwidth and storage  
- Are costly or invasive  
- Produce frequent false alarms (animals, wind)

**Objective:**  
Design a **low-power**, **privacy-first**, **edge-native** hardware node that can reliably detect **humans only**, without capturing or storing images.

Albaricoque is designed specifically for rural entrances, farm access points, fences, construction sites, and private properties needing **non-intrusive security**.

---

## ðŸ› ï¸ **Hardware Architecture**

| Component | Description |
|----------|-------------|
| **Arduino Nano 33 BLE Sense Rev2** | nRF52840 ARM Cortex-M4F, runs full TinyML inference |
| **4 Ã— PIR HC-SR501** | Direction, kinematics, activation deltas |
| **3 Ã— Ultrasonic HC-SR05 (15Â°/45Â°/75Â°)** | Distance, displacement, height approximation |
| **3D-printed Enclosure (PETG/TPU)** | Prototype, no IP rating; includes space for future LoRa antenna |
| **Level Shifters / Dividers** | Required since HC-SR05 echo output is 5 V |

**Prototype installation:**  
Mounted at **1.5 m height**, angled **90Â° forward** (initial 45Â° downward configuration replaced after field tests).

---

## ðŸ§© **Sensor Fusion Pipeline**

### PIR features
- Binary activation per sensor  
- Activation order  
- Î”t between PIR channels  
- Motion direction estimation  

### Ultrasonic features
- Sequential firing to avoid cross-talk  
- Median of burst readings  
- Outlier rejection  
- Baseline compensation  
- Distance deltas (15/45/75Â°)  
- Optional height estimation:  
```

h = H â€“ r Â· sin(Î¸)

```

### TinyML inference
- 6-class classifier  
- 1-D anomaly detection (K-means)  
- Real-time embedded inference (C++ SDK)

---

## ðŸ“Š **Dataset Description**

Custom dataset collected across:

- **Distances:** 0â€“3 m  
- **Azimuth:** â€“45Â° to +45Â°  
- **Velocities:** slow, normal, fast  
- **Postures:** standing, crouching, crawling  
- **Hard negatives:** dogs, wind-moved branches, rain  

### Input axes (7)
```

P1, P2, P3, P4, S1, S2, S3

```

### Window configuration
- Window size: **7000 ms**
- Stride: **1000 ms**
- Sampling freq: **3.57 Hz**
- Zero-padding: **enabled**

---

## ðŸ§  **Model Design (Impulse)**

### Processing Blocks
```

Time Series (7 axes)
â†“
Flatten
â†“
Classification (6 classes)
â†“
Anomaly Detection (K-means)

```

### Classes
- `Animal` â€” Activations generated by animals (e.g., dogs or other non-human movers) within the 0â€“3 m range.
- `Solo` â€” Background conditions with no human or animal present; ambient noise, wind-driven motion, or static environment.
- `Walk1` â€” Human walking at the closest range (~0â€“0.3 m).
- `Walk2` â€” Human walking at a short-to-medium range (~0.3â€“0.6 m).
- `Walk3` â€” Human walking at a medium-to-far range (~0.6â€“0.9 m).
- `Walk4` â€” Human walking at the farthest range (~0.9â€“1.2 m) still inside the sensorâ€™s coverage sector.

---

## ðŸ“ˆ **Model Performance**

| Metric | Value |
|--------|--------|
| **Accuracy** | **84.7%** |
| **Loss** | 0.37 |
| **Inference time** | **1 ms (on-device)** |
| **DSP time** | 0 ms |

### Validated Confusion Matrix (Summary)
- **Animal:** 100%  
- **Solo:** 89.7%  
- **Walk1:** 92.1%  
- **Walk3:** 81.3%  
- **Walk4:** 100%  
- **Walk2:** 62.5% (needs more samples)

---

## ðŸ§ª **Final System Logs & Analysis**

### Raw sample logs (excerpt)
```

TS(ms): 1430071 | sample: 0 | PIR: 1,0,0,1 | US(cm): 38,42,-1 | Delta(cm): 156.00,153.00,0.00
TS(ms): 1430348 | sample: 1 | PIR: 1,0,0,0 | US(cm): 32,-1,33 | Delta(cm): 162.00,0.00,164.00
...
TS(ms): 1436720 | sample: 24 | PIR: 0,0,0,0 | US(cm): 114,107,135 | Delta(cm): 80.00,88.00,62.00

```

### Statistical analysis
- Samples: **25**
- Duration: **6649 ms**
- Avg sampling interval: **277 ms**
- Outlier echoes: present (291â€“304 cm) due to multipath reflections
- PIR activation counts (25 samples):  
  - CH0=6, CH1=4, CH2=4, CH3=10

### Inference output example
```

Predictions (DSP: 0 ms., Classification: 1 ms.)
Animal: 0.00391
Solo: 0.00391
Walk1: 0.69922
Walk2: 0.21484
Walk3: 0.03516
Walk4: 0.03906
anomaly score: -0.867
Clase detectada: WALK1

````

---

## ðŸš€ **Deployment (Arduino + Edge Impulse)**

The project uses:

- **Edge Impulse C++ SDK**
- **PlatformIO (Arduino framework)**

Minimal inference snippet:
```cpp
signal_t signal;
numpy::signal_from_buffer(buffer, EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE, &signal);
run_classifier(&signal, &result, false);
````

Complete firmware is included in this repository under `/firmware`.

---

## ðŸ“¹ **Demo Video**

YouTube link:

[â–º Watch the build-and-demo video on YouTube](https://youtu.be/JdEEtX0hX48)

<details>
<summary>Embed (click to expand)</summary>

<iframe width="560" height="315" src="https://www.youtube.com/embed/JdEEtX0hX48" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

</details>

---

## ðŸ§­ **Project Impact**

### Innovation

* Full human detection without cameras
* Hybrid PIR/ultrasonic fusion
* Real-time TinyML on constrained MCU
* Privacy-by-design hardware

### Real-world impact

* Useful for rural properties, farms, private gates
* No personal data â†’ compliant with strict privacy norms
* Low power, low cost, high reliability
* Detects humans but ignores animals and foliage movement

---

## ðŸ”® **Future Work**

* Add LoRa/LoRaWAN uplink
* Improve ultrasonic sealing (IP65)
* Expand dataset for Walk2 class
* Multi-node perimeter mesh
* Weather-aware anomaly filtering

---

## ðŸ“‚ **Repository Structure**

```
.
â”œâ”€â”€ firmware/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ include/
â”‚   â””â”€â”€ platformio.ini
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â””â”€â”€ DATASET.md
â”œâ”€â”€ mechanical/
â”œâ”€â”€ hardware/
â”œâ”€â”€ edge-impulse/
â””â”€â”€ README.md
```

---

## ðŸ“„ **License**

Licensed under the **MIT License**.

---

## ðŸ™Œ **Acknowledgments**

* Edge Impulse Team
* TinyML & sensor fusion open-source community
* Mentors & reviewers from the Hackathon


